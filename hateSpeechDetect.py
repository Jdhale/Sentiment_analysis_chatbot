import pandas as pd
import re
import string
import re
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from indicnlp.normalize.indic_normalize import IndicNormalizerFactory
from indicnlp.tokenize import indic_tokenize

# === Load Data ===
train_df = pd.read_csv("Dataset/hate/hate_bin_train.csv")
valid_df = pd.read_csv("Dataset/hate/hate_bin_valid.csv")
test_df = pd.read_csv("Dataset/hate/hate_bin_test.csv")
civil_df = pd.read_csv("Dataset/hate/civil_hate_augmented.csv")
train_df = pd.concat([train_df, civil_df], ignore_index=True)

# === Normalizer for Marathi ===
normalizer = IndicNormalizerFactory().get_normalizer("mr")

# === Text Cleaning Function ===


def clean_text(text):
    # 1. Remove URLs
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)

    # 2. Remove mentions (@user) and hashtags
    text = re.sub(r'@\w+|#\w+', '', text)

    # 3. Remove emojis and symbols (including emojis in extended unicode)
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # Emoticons
        u"\U0001F300-\U0001F5FF"  # Symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # Transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # Flags
        u"\U00002700-\U000027BF"  # Dingbats
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)

    # 4. Remove all characters except Devanagari (Marathi) + spaces + basic punctuations
    # (Preserves full stops, commas, question marks etc.)
    text = re.sub(r'[^\u0900-\u097F\s.,!?]', '', text)

    # 5. Remove extra white spaces
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# Apply preprocessing
for df in [train_df, valid_df, test_df]:
    df['clean_text'] = df['text'].apply(clean_text)

# === Label Encoding ===
le = LabelEncoder()
train_df['label_enc'] = le.fit_transform(train_df['label'])
valid_df['label_enc'] = le.transform(valid_df['label'])
test_df['label_enc'] = le.transform(test_df['label'])

# === Build and Train Pipeline ===
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(
    ngram_range=(1, 3),       # include unigrams, bigrams, trigrams
    max_features=10000,       # allow richer vocabulary
    min_df=2,                 # ignore rare terms
    sublinear_tf=True         # dampens term frequency influence
)
),
    ('clf', MultinomialNB())
])
pipeline.fit(train_df['clean_text'], train_df['label_enc'])

# === Evaluate ===
val_preds = pipeline.predict(valid_df['clean_text'])
test_preds = pipeline.predict(test_df['clean_text'])

print("üìä Validation Report:")
print(classification_report(valid_df['label_enc'], val_preds, target_names=le.classes_))

print("\nüìä Test Report:")
print(classification_report(test_df['label_enc'], test_preds, target_names=le.classes_))

marathi_hate_sentences = [
    "‡§Æ‡§≤‡§æ ‡§ñ‡•ã‡§ü‡•á‡§™‡§£‡§æ ‡§Æ‡•Å‡§≥‡•Ä‡§ö ‡§Ü‡§µ‡§°‡§§ ‡§®‡§æ‡§π‡•Ä, ‡§Æ‡•Ä ‡§§‡•ç‡§Ø‡§æ‡§ö‡§æ ‡§§‡§ø‡§∞‡§∏‡•ç‡§ï‡§æ‡§∞ ‡§ï‡§∞‡§§‡•ã.",
    "‡§§‡•ç‡§Ø‡§æ‡§ö‡§Ç ‡§µ‡§æ‡§ó‡§£‡§Ç ‡§™‡§æ‡§π‡•Ç‡§® ‡§ö‡•Ä‡§°‡§ö ‡§Ø‡•á‡§§‡•á.",
    "‡§Æ‡§æ‡§ù‡§æ ‡§∞‡§æ‡§ó ‡§Ö‡§®‡§æ‡§µ‡§∞ ‡§π‡•ã‡§§‡•ã ‡§ú‡•á‡§µ‡•ç‡§π‡§æ ‡§ï‡•ã‡§£‡•Ä ‡§ñ‡•ã‡§ü‡§Ç ‡§¨‡•ã‡§≤‡§§‡§Ç.",
    "‡§Ö‡§∏‡•á ‡§≤‡•ã‡§ï ‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ ‡§¶‡•É‡§∑‡•ç‡§ü‡•Ä‡§®‡§Ç ‡§ò‡•É‡§£‡§æ‡§∏‡•ç‡§™‡§¶ ‡§Ü‡§π‡•á‡§§.",
    "‡§Æ‡§≤‡§æ ‡§Ö‡§∂‡§æ ‡§≤‡•ã‡§ï‡§æ‡§Ç‡§ö‡§æ ‡§§‡§ø‡§∞‡§∏‡•ç‡§ï‡§æ‡§∞ ‡§µ‡§æ‡§ü‡§§‡•ã ‡§ú‡•á ‡§á‡§§‡§∞‡§æ‡§Ç‡§®‡§æ ‡§§‡•ç‡§∞‡§æ‡§∏ ‡§¶‡•á‡§§‡§æ‡§§.",
    "‡§§‡§ø‡§ö‡§Ç ‡§µ‡§æ‡§ó‡§£‡§Ç ‡§á‡§§‡§ï‡§Ç ‡§Ö‡§π‡§Ç‡§ï‡§æ‡§∞‡•Ä ‡§Ü‡§π‡•á ‡§ï‡•Ä ‡§∏‡§π‡§®‡§ö ‡§π‡•ã‡§§ ‡§®‡§æ‡§π‡•Ä.",
    "‡§π‡•á ‡§∏‡§ó‡§≥‡§Ç ‡§™‡§æ‡§π‡•Ç‡§® ‡§Æ‡§≤‡§æ ‡§ò‡•É‡§£‡§æ ‡§µ‡§æ‡§ü‡§§‡•á.",
    "‡§§‡•Å ‡§ñ‡•Ç‡§™‡§ö ‡§Ö‡§∏‡§≠‡•ç‡§Ø ‡§Ü‡§£‡§ø ‡§â‡§¶‡•ç‡§ß‡§ü ‡§Ü‡§π‡•á‡§∏.",
    "‡§Æ‡§æ‡§ù‡§Ç ‡§§‡•ç‡§Ø‡§æ‡§ö‡•ç‡§Ø‡§æ‡§µ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏ ‡§®‡§æ‡§π‡•Ä ‡§ï‡§æ‡§∞‡§£ ‡§§‡•ã ‡§è‡§ï ‡§¨‡§®‡§µ‡§æ‡§¨‡§®‡§µ‡•Ä ‡§ï‡§∞‡§£‡§æ‡§∞‡§æ ‡§Ü‡§π‡•á.",
    "‡§π‡§æ ‡§Æ‡§æ‡§£‡•Ç‡§∏ ‡§∏‡§Æ‡§æ‡§ú‡§æ‡§∏‡§æ‡§†‡•Ä ‡§ß‡•ã‡§ï‡§æ‡§¶‡§æ‡§Ø‡§ï ‡§Ü‡§π‡•á, ‡§Ø‡§æ‡§≤‡§æ ‡§•‡§æ‡§Ç‡§¨‡§µ‡§≤‡§Ç ‡§™‡§æ‡§π‡§ø‡§ú‡•á."
]

for sentence in marathi_hate_sentences:
    cleaned_sentence = clean_text(sentence)
    prediction = pipeline.predict([cleaned_sentence])[0]
    label_pred = le.inverse_transform([prediction])[0]
    print(f"Input Sentence: {sentence}")
    print(f"Predicted Label: {label_pred}")
    print("-" * 50)

# # Example: Check a custom Marathi sentence
# custom_sentence = "‡§Æ‡§≤‡§æ ‡§ñ‡•ã‡§ü‡•á‡§™‡§£‡§æ ‡§Æ‡•Å‡§≥‡•Ä‡§ö ‡§Ü‡§µ‡§°‡§§ ‡§®‡§æ‡§π‡•Ä, ‡§Æ‡•Ä ‡§§‡•ç‡§Ø‡§æ‡§ö‡§æ ‡§§‡§ø‡§∞‡§∏‡•ç‡§ï‡§æ‡§∞ ‡§ï‡§∞‡§§‡•ã."  # Replace with your test sentence
# cleaned_sentence = clean_text(custom_sentence)
# prediction = pipeline.predict([cleaned_sentence])[0]

# # Decode the prediction
# label_pred = le.inverse_transform([prediction])[0]
# print(f"Input Sentence: {custom_sentence}")
# print(f"Predicted Label: {label_pred}")
